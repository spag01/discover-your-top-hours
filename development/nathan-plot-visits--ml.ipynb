{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer: some of the code here reuses code Nathan submitted for CMPT 353, Excercise 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from custom_process_domain import process_domain_normal\n",
    "import matplotlib.pyplot as plt\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSSIBLE_USERS = { # Just to make sure we're good\n",
    "  \"Juan\": \"Juan\",\n",
    "  \"Nathan\": \"Nathan\",\n",
    "  \"Sanyam\": \"Sanyam\",\n",
    "  \"Nipun\": \"Nipun\",\n",
    "}\n",
    "\n",
    "# User Input\n",
    "USER = POSSIBLE_USERS[\"Nathan\"]\n",
    "\n",
    "# START_DATE\n",
    "START_DATE_YEAR = 2022\n",
    "START_DATE_MONTH = 9\n",
    "START_DATE_DAY = 26\n",
    "\n",
    "WEEKS_TO_INCREMENT = -1 # Add WEEKS_TO_INCREMENT * 7 days to START_TIME\n",
    "DATE_INTERVAL_TYPE = 'month' # 'weekend', 'week', 'all_week', 'all_time\n",
    "\n",
    "##############################\n",
    "# Graphs\n",
    "##############################\n",
    "\n",
    "# Save Graphs\n",
    "SAVE_GRAPH_1 = False\n",
    "SAVE_GRAPH_2 = False\n",
    "SAVE_GRAPH_3 = True\n",
    "\n",
    "##############################\n",
    "# Options\n",
    "##############################\n",
    "MIN_CHROME_VISIT_DURATION = 5\n",
    "MAX_CHROME_VISIT_DURATION = 2.88e+10 # 8 hours in microseconds\n",
    "\n",
    "MIN_SAFARI_SCORE = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config per User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USER == \"Nathan\":\n",
    "  DB_FILE = \"../data/history--2022-10-21--Nathan-Tsai.sqlite3\"\n",
    "  RATING_FILE = \"../rated-output/Ratings - CMPT 353 - Nathan Tsai (1).csv\"\n",
    "  USER_TIMEZONE_STRING = 'US/Pacific'\n",
    "elif USER == \"Juan\":\n",
    "  DB_FILE = \"../data/history--2022-10-21--Juan-Gonzalez.sqlite3\"\n",
    "  RATING_FILE = \"../rated-output/Ratings - CMPT 353 - Juan Gonzalez.csv\"\n",
    "  USER_TIMEZONE_STRING = 'US/Pacific'\n",
    "elif USER == \"Sanyam\":\n",
    "  DB_FILE = \"../data/history--2022-11-25--Sanyam-Safari.db\"\n",
    "  RATING_FILE = \"../rated-output/Ratings of Inputted Users - CMPT 353 - Sanyam (2).csv\"\n",
    "  USER_TIMEZONE_STRING = 'US/Pacific'\n",
    "elif USER == \"Nipun\":\n",
    "  DB_FILE = \"../data/history--2022-11-26--Nipun-Safari.db\"\n",
    "  RATING_FILE = \"../rated-output/Ratings of Inputted Users - CMPT 353 - Nipun.csv\"\n",
    "  USER_TIMEZONE_STRING = 'Asia/Colombo' # India standard time: UTC+5:30\n",
    "else:\n",
    "  assert(False and \"User name is invalid\")\n",
    "\n",
    "USER_TIMEZONE = ZoneInfo(USER_TIMEZONE_STRING)\n",
    "TIMEZONE_STRING = USER_TIMEZONE_STRING\n",
    "\n",
    "\n",
    "START_TIME = datetime(\n",
    "  START_DATE_YEAR,\n",
    "  START_DATE_MONTH,\n",
    "  START_DATE_DAY, \n",
    "  tzinfo=USER_TIMEZONE\n",
    ") # the data to look at. Should be a Monday\n",
    "\n",
    "# The values per user productivity\n",
    "productivity_to_int_map = {\n",
    "  'Always Distracted' : -2,\n",
    "  'Mostly Distracted' : -1,\n",
    "  'Neutrel' : 0,\n",
    "  'Mostly Intentional' : 1,\n",
    "  'Always Intentional' : 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format User Input for Graphs / Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increment by a number of weeks\n",
    "START_TIME = START_TIME + timedelta(days=7 * WEEKS_TO_INCREMENT)\n",
    "\n",
    "# Add 1 day if weekend, otherwise use weekday\n",
    "date_types = {\n",
    "  'weekend': 1,\n",
    "  'week': 4,\n",
    "  'all_week': 6,\n",
    "  'month': 30,\n",
    "  'all_time': 7 * 52 * 3, # 3 years\n",
    "}\n",
    "DAYS_TO_ADD = date_types[DATE_INTERVAL_TYPE]\n",
    "\n",
    "if DATE_INTERVAL_TYPE == 'weekend':\n",
    "  START_TIME = START_TIME + timedelta(days=-2)\n",
    "\n",
    "END_TIME = START_TIME + timedelta(days=DAYS_TO_ADD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATE_INTERVAL_TYPE == 'all_time':\n",
    "  start_date_for_graph = START_TIME.strftime('%a, %b %d %Y')\n",
    "else:\n",
    "  start_date_for_graph = START_TIME.strftime('%a, %b %d')\n",
    "\n",
    "  \n",
    "end_date_for_graph = END_TIME.strftime('%a, %b %d, %Y')\n",
    "time_for_graph = \"{} - {}\".format(start_date_for_graph, end_date_for_graph)\n",
    "CUSTOM_PARAMS_FOR_GRAPH = \"{}: {}\".format(USER, time_for_graph)\n",
    "\n",
    "start_date_for_file = START_TIME.strftime('%b-%d-%Y')\n",
    "end_date_for_file = END_TIME.strftime('%b-%d-%Y')\n",
    "time_for_file = \"{}--{}\".format(start_date_for_file, end_date_for_file)\n",
    "CUSTOM_PARAMS_FOR_FILE = \"{}--{}\".format(USER, time_for_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "WINDOWS_EPOCH_MICROSECS      = -11644473600000 * 1000\n",
    "SAFARI_TIME_UPDATE           = 978307200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the type of Database\n",
    "TYPE_IS_SAFARI = \"TYPE_IS_SAFARI\"\n",
    "TYPE_IS_CHROME = \"TYPE_IS_CHROME\"\n",
    "TYPE_OF_DB = {\n",
    "  \"db\": TYPE_IS_SAFARI,\n",
    "  \"sqlite\": TYPE_IS_CHROME,\n",
    "  \"sqlite3\": TYPE_IS_CHROME,\n",
    "}\n",
    "\n",
    "extension = DB_FILE.split(\".\")[-1]\n",
    "DB_TYPE = TYPE_OF_DB[extension]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_INCREMENTS = 24 * 2\n",
    "DAY_IN_SECONDS = 60 * 60 * 24\n",
    "ROUND_TO = DAY_IN_SECONDS / NUMBER_OF_INCREMENTS\n",
    "\n",
    "def get_half_hour(data: pd.Series) -> pd.Series:\n",
    "  return (data / ROUND_TO).round().astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bunch of Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load History Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DB_TYPE == TYPE_IS_CHROME:\n",
    "  query_get_urls_and_times = \"\"\"\n",
    "  SELECT v.id, v.visit_time, v.visit_duration, u.url\n",
    "  FROM 'visits' as v \n",
    "  LEFT JOIN urls u ON u.id = v.url\n",
    "  \"\"\"\n",
    "\n",
    "  TIME_CORRECTION_TO_ADD_TO_VISIT_TIME = WINDOWS_EPOCH_MICROSECS\n",
    "elif DB_TYPE == TYPE_IS_SAFARI:\n",
    "  query_get_urls_and_times = \"\"\"\n",
    "  SELECT v.id, v.visit_time, v.score, u.url\n",
    "  FROM 'history_visits' as v \n",
    "  LEFT JOIN history_items u ON u.id = v.history_item\n",
    "  \"\"\"\n",
    "  TIME_CORRECTION_TO_ADD_TO_VISIT_TIME = SAFARI_TIME_UPDATE\n",
    "else:\n",
    "  assert(False and \"Extension of database is invalid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(DB_FILE) as con:\n",
    "  visits = pd.read_sql_query(query_get_urls_and_times, con)\n",
    "\n",
    "visits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit time in microseconds (s/1,000,000)\n",
    "# https://chromium.googlesource.com/chromium/src/+/lkgr/base/time/time.h\n",
    "\n",
    "if DB_TYPE == TYPE_IS_CHROME:\n",
    "  visit_time_in_ns = (visits['visit_time'] + TIME_CORRECTION_TO_ADD_TO_VISIT_TIME) * 1000\n",
    "  visits['visit_time_epoch'] = pd.to_datetime(visit_time_in_ns, unit='ns', utc=True).map(lambda x: x.tz_convert(TIMEZONE_STRING))\n",
    "elif DB_TYPE == TYPE_IS_SAFARI:\n",
    "  visit_time_in_ns = (visits['visit_time'] + TIME_CORRECTION_TO_ADD_TO_VISIT_TIME)\n",
    "  visits['visit_time_epoch'] = pd.to_datetime(visit_time_in_ns, unit='s', utc=True).map(lambda x: x.tz_convert(TIMEZONE_STRING))\n",
    "else:\n",
    "  assert(False and \"Extension of database is invalid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits['domain'] = visits['url'].apply(process_domain_normal)\n",
    "\n",
    "visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits.tail(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(RATING_FILE, index_col='domain')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter the Data based on Time + Visit Duration/Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greaterthanStartTime = visits['visit_time_epoch'] >= START_TIME\n",
    "lessThanEndTime = visits['visit_time_epoch'] <= END_TIME\n",
    "visits_this_semester = visits[greaterthanStartTime & lessThanEndTime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'visit_duration' in visits_this_semester.columns:\n",
    "  min_visit_option = visits_this_semester[visits_this_semester['visit_duration'] >= MIN_CHROME_VISIT_DURATION]\n",
    "  min_visit_option = min_visit_option[min_visit_option['visit_duration'] < MAX_CHROME_VISIT_DURATION]\n",
    "elif 'score' in visits_this_semester.columns:\n",
    "  min_visit_option = visits_this_semester[visits_this_semester['score'] >= MIN_SAFARI_SCORE]\n",
    "else:\n",
    "  min_visit_option = visits_this_semester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_data = min_visit_option.join(ratings[['manual_rating']], on=\"domain\", how='left')\n",
    "final_data['productivity_scale'] = final_data['manual_rating'].map(productivity_to_int_map)\n",
    "final_data['time_of_day'] = final_data['visit_time_epoch'].dt.hour * 60 * 60 + final_data['visit_time_epoch'].dt.minute * 60 + final_data['visit_time_epoch'].dt.second\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill empty productivity scores with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove N/A\n",
    "refined_data = final_data.copy()\n",
    "refined_data['productivity_scale'] = refined_data['productivity_scale'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Account for Visit Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HALF_HOUR = 60 * 30\n",
    "\n",
    "refined_data['adjusted_time'] = refined_data[['visit_duration', 'visit_time_epoch']].apply(\n",
    "  lambda x: np.arange(0, x['visit_duration'] // 1e6, HALF_HOUR) + x['visit_time_epoch'].value // 1e9, axis=1)\n",
    "\n",
    "adjusted_data = refined_data.explode('adjusted_time')\n",
    "adjusted_data.dropna(subset=['adjusted_time'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_data['adjusted_datetime'] = pd.to_datetime(adjusted_data['adjusted_time'], unit='s', utc=True).map(lambda x: x.tz_convert(TIMEZONE_STRING))\n",
    "\n",
    "adjusted_data['time_of_day'] = adjusted_data['adjusted_datetime'].dt.hour * 60 * 60 \\\n",
    "  + adjusted_data['adjusted_datetime'].dt.minute * 60 \\\n",
    "  + adjusted_data['adjusted_datetime'].dt.second\n",
    "\n",
    "adjusted_data['half_hour'] = get_half_hour(adjusted_data['time_of_day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_data['adjusted_prod'] = adjusted_data['productivity_scale']\n",
    "positive_count = adjusted_data[adjusted_data['productivity_scale'] > 0].size\n",
    "adjusted_data.loc[adjusted_data['productivity_scale'] > 0, 'adjusted_prod'] /= positive_count\n",
    "\n",
    "negative_count = adjusted_data[adjusted_data['productivity_scale'] < 0].size\n",
    "adjusted_data.loc[adjusted_data['productivity_scale'] < 0, 'adjusted_prod'] /= negative_count\n",
    "\n",
    "# adjusted_data_agg = adjusted_data.groupby('half_hour').agg(sum).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = adjusted_data[['half_hour']]\n",
    "y = adjusted_data['adjusted_prod']\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_pre = {}\n",
    "\n",
    "models_pre['kneighbours_model'] = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    KNeighborsRegressor(n_neighbors=10)\n",
    ")\n",
    "\n",
    "models_pre[\"rf_model_12_20\"] = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    RandomForestRegressor(\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.DataFrame({\n",
    "  'name': models_pre.keys(),\n",
    "  'model': models_pre.values(),\n",
    "})\n",
    "\n",
    "models['model_trained'] = models['model'].apply(lambda x : x.fit(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kneighbours_model\n",
      "rf_model_12_20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>model</th>\n",
       "      <th>model_trained</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kneighbours_model</td>\n",
       "      <td>(StandardScaler(), KNeighborsRegressor(n_neigh...</td>\n",
       "      <td>(StandardScaler(), KNeighborsRegressor(n_neigh...</td>\n",
       "      <td>(1.0, 1.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rf_model_12_20</td>\n",
       "      <td>(StandardScaler(), (DecisionTreeRegressor(max_...</td>\n",
       "      <td>(StandardScaler(), (DecisionTreeRegressor(max_...</td>\n",
       "      <td>(1.0, 1.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name                                              model  \\\n",
       "0  kneighbours_model  (StandardScaler(), KNeighborsRegressor(n_neigh...   \n",
       "1     rf_model_12_20  (StandardScaler(), (DecisionTreeRegressor(max_...   \n",
       "\n",
       "                                       model_trained       score  \n",
       "0  (StandardScaler(), KNeighborsRegressor(n_neigh...  (1.0, 1.0)  \n",
       "1  (StandardScaler(), (DecisionTreeRegressor(max_...  (1.0, 1.0)  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_model_scores(row):\n",
    "  print(row['name'])\n",
    "  score_train = row['model_trained'].score(X_train, y_train)\n",
    "  score_val = row['model_trained'].score(X_valid, y_valid)\n",
    "\n",
    "  return (score_train, score_val)\n",
    "\n",
    "models['score'] = models.apply(print_model_scores, axis=1)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\natha\\anaconda3\\envs\\dataScience\\lib\\site-packages\\pandas\\core\\indexes\\range.py:385\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 385\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_range\u001b[39m.\u001b[39;49mindex(new_key)\n\u001b[0;32m    386\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[1;31mValueError\u001b[0m: 2 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [26], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m x_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, \u001b[39m48\u001b[39m)\n\u001b[0;32m      3\u001b[0m x_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\n\u001b[0;32m      4\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mhalf_hour\u001b[39m\u001b[39m\"\u001b[39m: x_data\n\u001b[0;32m      5\u001b[0m })\n\u001b[1;32m----> 6\u001b[0m y_data \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39;49mloc[\u001b[39m2\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mmodel_trained\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mpredict(x_df)\n",
      "File \u001b[1;32mc:\\Users\\natha\\anaconda3\\envs\\dataScience\\lib\\site-packages\\pandas\\core\\indexing.py:960\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    958\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(com\u001b[39m.\u001b[39mapply_if_callable(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m key)\n\u001b[0;32m    959\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m--> 960\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_get_value(\u001b[39m*\u001b[39;49mkey, takeable\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_takeable)\n\u001b[0;32m    961\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_tuple(key)\n\u001b[0;32m    962\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\natha\\anaconda3\\envs\\dataScience\\lib\\site-packages\\pandas\\core\\frame.py:3622\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[1;34m(self, index, col, takeable)\u001b[0m\n\u001b[0;32m   3616\u001b[0m engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_engine\n\u001b[0;32m   3618\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, MultiIndex):\n\u001b[0;32m   3619\u001b[0m     \u001b[39m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[0;32m   3620\u001b[0m     \u001b[39m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[0;32m   3621\u001b[0m     \u001b[39m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n\u001b[1;32m-> 3622\u001b[0m     row \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(index)\n\u001b[0;32m   3623\u001b[0m     \u001b[39mreturn\u001b[39;00m series\u001b[39m.\u001b[39m_values[row]\n\u001b[0;32m   3625\u001b[0m \u001b[39m# For MultiIndex going through engine effectively restricts us to\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m \u001b[39m#  same-length tuples; see test_get_set_value_no_partial_indexing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\natha\\anaconda3\\envs\\dataScience\\lib\\site-packages\\pandas\\core\\indexes\\range.py:387\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_range\u001b[39m.\u001b[39mindex(new_key)\n\u001b[0;32m    386\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 387\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    389\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "x_data = np.arange(0, 48)\n",
    "\n",
    "x_df = pd.DataFrame({\n",
    "  \"half_hour\": x_data\n",
    "})\n",
    "y_data = models.loc[2, 'model_trained'].predict(x_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(x_df['half_hour'], y_data, 'r-')\n",
    "plt.plot(x_df['half_hour'], y_data, 'r.')\n",
    "plt.axhline(y=0, color='b', linestyle='--', alpha=0.5)\n",
    "plt.xticks(np.arange(0, NUMBER_OF_INCREMENTS, NUMBER_OF_INCREMENTS/24), np.arange(0, 24, step=1)) # Half Hours\n",
    "plt.title(\"Estimated productivity using Random Forest: \\n ({})\".format(CUSTOM_PARAMS_FOR_GRAPH))\n",
    "plt.xlabel(\"Time (Hour)\")\n",
    "plt.ylabel(\"Sum of Productivity Level (-1 for Distracted. -2 for Very Distracted)\")\n",
    "\n",
    "if SAVE_GRAPH_3:\n",
    "  plt.savefig(\"../plots/XX-ml--SOME-MODEL-predictions--{}\".format(CUSTOM_PARAMS_FOR_FILE))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "24e4cd118faed2a7f4ff0294d1a1b4521466fc1fb850bf8e754b6eda5f0ef937"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
